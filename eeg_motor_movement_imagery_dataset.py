# -*- coding: utf-8 -*-
"""EEG Motor Movement/Imagery Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oBnwmh1-T8FYozsem7Va-PLmXGNFTi74
"""

!pip install mne

from google.colab import drive
drive.mount('/content/gdrive')

'/content/gdrive/My Drive/eegbci_data'

########################--------------------2 classes----------------------------------################################
import numpy as np
import matplotlib.pyplot as plt
import mne
from mne.datasets import eegbci
from mne.io import read_raw_edf
from mne.decoding import CSP
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, accuracy_score
from sklearn.pipeline import Pipeline
from collections import Counter
import os
import pandas as pd

# Download dataset for left/right hand motor imagery (runs 4, 8, 12)
#eegbci.load_data(subjects=[1,50], runs=[4, 8, 12], path='./physionet_mi', update_path=True)
eegbci.load_data(subjects=list(range(1, 51)), runs=[4, 8, 12], path='./physionet_mi', update_path=True)
# List downloaded files
files = []
for root, dirs, filenames in os.walk('./physionet_mi'):
    for filename in filenames:
        if filename.endswith('.edf'):
            files.append(os.path.join(root, filename))
files.sort()
print(f"Found {len(files)} EDF files")

# Preprocessing parameters
low_cut = 8.0    # Mu/Beta bands
high_cut = 30.0
tmin = 1.0       # Start after cue (avoid initial artifact)
tmax = 4.0       # End before rest
sfreq = 160      # Sampling rate (Hz)
n_components = 4 # For CSP

def preprocess_data(files, low_cut, high_cut, tmin, tmax, sfreq):
    all_epochs = []
    all_labels = []
    left_count = 0
    right_count = 0

    for file in files:
        raw = read_raw_edf(file, preload=True)
        raw.filter(low_cut, high_cut, fir_design='firwin')
        events, event_mapping = mne.events_from_annotations(raw)

        # Count class distribution
        for event in events:
            if event[2] == event_mapping.get('T1', -1):
                left_count += 1
            elif event[2] == event_mapping.get('T2', -1):
                right_count += 1

        # Create epochs
        event_id = {k: v for k, v in event_mapping.items() if k in ['T1', 'T2']}
        if not event_id:
            continue

        epochs = mne.Epochs(raw, events, event_id=event_id,
                           tmin=tmin, tmax=tmax, baseline=None, preload=True)

        if len(epochs) > 0:
            X = epochs.get_data()
            y = epochs.events[:, 2]
            all_epochs.append(X)
            all_labels.append(y)

    if not all_epochs:
        raise ValueError("No valid epochs found in any files")

    X = np.concatenate(all_epochs, axis=0)
    y = np.concatenate(all_labels, axis=0)

    available_classes = sorted(set(y))
    class_mapping = {}
    for event_value in available_classes:
        if event_value == event_mapping.get('T1', -1):
            class_mapping[event_value] = 'Left Hand'
        elif event_value == event_mapping.get('T2', -1):
            class_mapping[event_value] = 'Right Hand'

    return X, y, available_classes, class_mapping, epochs, left_count, right_count

# Load and preprocess data
X, y, available_classes, class_mapping, last_epochs, left_count, right_count = preprocess_data(
    files, low_cut, high_cut, tmin, tmax, sfreq)

# Print dataset information
print(f"\nDataset shape: {X.shape} (trials × channels × time)")
print(f"Class counts: Left Hand: {left_count}, Right Hand: {right_count}")

# 1. Visualize class distribution as pie chart
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
labels = ['Left Hand', 'Right Hand']
sizes = [left_count, right_count]
colors = ['#ff9999', '#66b3ff']
explode = (0.05, 0.05)

plt.pie(sizes, explode=explode, labels=labels, colors=colors,
        autopct=lambda p: f'{p:.1f}%\n({int(p*sum(sizes)/100)} trials)',
        shadow=True, startangle=90, textprops={'fontsize': 10})
plt.title('Class Distribution', pad=20)

# 2. Plot ERPs
plt.subplot(1, 2, 2)
for label in available_classes:
    plt.plot(X[y == label].mean(axis=0).T,
             label=f'{class_mapping.get(label, str(label))}')
plt.title("Event-Related Potentials")
plt.xlabel("Time (samples)")
plt.ylabel("Amplitude (μV)")
plt.legend()
plt.tight_layout()
plt.show()

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define classifiers to compare
classifiers = {
    'LDA': LinearDiscriminantAnalysis(),
    'SVM': SVC(kernel='linear', C=1.0, probability=True),
    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42),
    'k-NN': KNeighborsClassifier(n_neighbors=5)
}

# Evaluate each classifier
results = []
for name, clf in classifiers.items():
    # Create pipeline with CSP and classifier
    pipeline = Pipeline([
        ('CSP', CSP(n_components=n_components, reg=None, log=True, norm_trace=False)),
        ('Classifier', clf)
    ])

    # Train and evaluate
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # Cross-validation
    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')

    # Store results
    results.append({
        'Classifier': name,
        'Test Accuracy': accuracy,
        'CV Mean Accuracy': np.mean(cv_scores),
        'CV Std': np.std(cv_scores)
    })

    # Print classification report
    print(f"\n{name} Classification Report:")
    print(classification_report(y_test, y_pred, target_names=[class_mapping[c] for c in sorted(available_classes)]))

# Convert results to DataFrame for nice display
results_df = pd.DataFrame(results)
print("\nClassifier Performance Comparison:")
print(results_df[['Classifier', 'Test Accuracy', 'CV Mean Accuracy', 'CV Std']])

# Plot classifier comparison
plt.figure(figsize=(10, 6))
x = np.arange(len(classifiers))
width = 0.35

plt.bar(x - width/2, results_df['Test Accuracy'], width, label='Test Accuracy')
plt.bar(x + width/2, results_df['CV Mean Accuracy'], width,
        yerr=results_df['CV Std'], label='CV Accuracy ± std')

plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Classifier Performance Comparison')
plt.xticks(x, classifiers.keys())
plt.ylim(0.5, 1.0)
plt.legend()
plt.tight_layout()
plt.show()

# Visualize CSP patterns from the best classifier
best_idx = results_df['Test Accuracy'].idxmax()
best_name = results_df.loc[best_idx, 'Classifier']
best_clf = classifiers[best_name]

print(f"\nBest classifier: {best_name}")

# Fit best pipeline to visualize CSP patterns
best_pipeline = Pipeline([
    ('CSP', CSP(n_components=n_components, reg=None, log=True, norm_trace=False)),
    ('Classifier', best_clf)
])
best_pipeline.fit(X_train, y_train)

# Plot CSP patterns
#best_pipeline.named_steps['CSP'].plot_patterns(last_epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)
plt.suptitle(f'CSP Patterns ({best_name} Classifier)')
plt.show()

######################------- 3classes----------------------------##################################
import numpy as np
import matplotlib.pyplot as plt
import mne
from mne.datasets import eegbci
from mne.io import read_raw_edf
from mne.decoding import CSP
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, accuracy_score
from sklearn.pipeline import Pipeline
from collections import Counter
import os
import pandas as pd

# Download dataset - including rest runs (3, 7, 11) along with motor imagery runs (4, 8, 12)
#eegbci.load_data(subjects=[1,50], runs=[4, 8, 12], path='./physionet_mi', update_path=True)
eegbci.load_data(subjects=list(range(1, 51)), runs=[4, 8, 12], path='./physionet_mi', update_path=True)
# List downloaded files
files = []
for root, dirs, filenames in os.walk('./physionet_mi'):
    for filename in filenames:
        if filename.endswith('.edf'):
            files.append(os.path.join(root, filename))
files.sort()
print(f"Found {len(files)} EDF files")

# Preprocessing parameters
low_cut = 8.0    # Mu/Beta bands
high_cut = 30.0
tmin = 1.0       # Start after cue (avoid initial artifact)
tmax = 4.0       # End before rest
sfreq = 160      # Sampling rate (Hz)
n_components = 4 # For CSP

def preprocess_data(files, low_cut, high_cut, tmin, tmax, sfreq):
    all_epochs = []
    all_labels = []
    left_count = 0
    right_count = 0
    rest_count = 0

    for file in files:
        raw = read_raw_edf(file, preload=True)
        raw.filter(low_cut, high_cut, fir_design='firwin')
        events, event_mapping = mne.events_from_annotations(raw)

        # Count class distribution (T1: Left, T2: Right, T0: Rest)
        for event in events:
            if event[2] == event_mapping.get('T1', -1):
                left_count += 1
            elif event[2] == event_mapping.get('T2', -1):
                right_count += 1
            elif event[2] == event_mapping.get('T0', -1):
                rest_count += 1

        # Create epochs for all three classes
        event_id = {k: v for k, v in event_mapping.items() if k in ['T0', 'T1', 'T2']}
        if not event_id:
            continue

        epochs = mne.Epochs(raw, events, event_id=event_id,
                           tmin=tmin, tmax=tmax, baseline=None, preload=True)

        if len(epochs) > 0:
            X = epochs.get_data()
            y = epochs.events[:, 2]
            all_epochs.append(X)
            all_labels.append(y)

    if not all_epochs:
        raise ValueError("No valid epochs found in any files")

    X = np.concatenate(all_epochs, axis=0)
    y = np.concatenate(all_labels, axis=0)

    available_classes = sorted(set(y))
    class_mapping = {}
    for event_value in available_classes:
        if event_value == event_mapping.get('T0', -1):
            class_mapping[event_value] = 'Rest'
        elif event_value == event_mapping.get('T1', -1):
            class_mapping[event_value] = 'Left Hand'
        elif event_value == event_mapping.get('T2', -1):
            class_mapping[event_value] = 'Right Hand'

    return X, y, available_classes, class_mapping, epochs, left_count, right_count, rest_count

# Load and preprocess data
X, y, available_classes, class_mapping, last_epochs, left_count, right_count, rest_count = preprocess_data(
    files, low_cut, high_cut, tmin, tmax, sfreq)

# Print dataset information
print(f"\nDataset shape: {X.shape} (trials × channels × time)")
print(f"Class counts: Rest: {rest_count}, Left Hand: {left_count}, Right Hand: {right_count}")

# 1. Visualize class distribution as pie chart
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
labels = ['Rest', 'Left Hand', 'Right Hand']
sizes = [rest_count, left_count, right_count]
colors = ['#dddddd', '#ff9999', '#66b3ff']
explode = (0.05, 0.05, 0.05)  # Explode all slices for better visibility

plt.pie(sizes, explode=explode, labels=labels, colors=colors,
        autopct=lambda p: f'{p:.1f}%\n({int(p*sum(sizes)/100)} trials)',
        shadow=True, startangle=90, textprops={'fontsize': 10})
plt.title('Class Distribution (3 Classes)', pad=20)

# 2. Plot ERPs for all three classes
plt.subplot(1, 2, 2)
for label in available_classes:
    plt.plot(X[y == label].mean(axis=0).T,
             label=f'{class_mapping.get(label, str(label))}')
plt.title("Event-Related Potentials (3 Classes)")
plt.xlabel("Time (samples)")
plt.ylabel("Amplitude (μV)")
plt.legend()
plt.tight_layout()
plt.show()

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define classifiers to compare (now for 3-class problem)
classifiers = {
    'LDA': LinearDiscriminantAnalysis(),
    'SVM': SVC(kernel='linear', C=1.0, probability=True),
    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42),
    'k-NN': KNeighborsClassifier(n_neighbors=5)
}

# Evaluate each classifier
results = []
for name, clf in classifiers.items():
    # Create pipeline with CSP and classifier
    pipeline = Pipeline([
        ('CSP', CSP(n_components=n_components, reg=None, log=True, norm_trace=False)),
        ('Classifier', clf)
    ])

    # Train and evaluate
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # Cross-validation
    cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')

    # Store results
    results.append({
        'Classifier': name,
        'Test Accuracy': accuracy,
        'CV Mean Accuracy': np.mean(cv_scores),
        'CV Std': np.std(cv_scores)
    })

    # Print classification report
    print(f"\n{name} Classification Report:")
    print(classification_report(y_test, y_pred,
                              target_names=[class_mapping[c] for c in sorted(available_classes)]))

# Convert results to DataFrame for nice display
results_df = pd.DataFrame(results)
print("\nClassifier Performance Comparison (3 Classes):")
print(results_df[['Classifier', 'Test Accuracy', 'CV Mean Accuracy', 'CV Std']])

# Plot classifier comparison
plt.figure(figsize=(10, 6))
x = np.arange(len(classifiers))
width = 0.35

plt.bar(x - width/2, results_df['Test Accuracy'], width, label='Test Accuracy')
plt.bar(x + width/2, results_df['CV Mean Accuracy'], width,
        yerr=results_df['CV Std'], label='CV Accuracy ± std')

plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Classifier Performance Comparison (3 Classes)')
plt.xticks(x, classifiers.keys())
plt.ylim(0.0, 1.0)  # Adjusted scale for potentially lower 3-class accuracy
plt.legend()
plt.tight_layout()
plt.show()

# Visualize CSP patterns from the best classifier
best_idx = results_df['Test Accuracy'].idxmax()
best_name = results_df.loc[best_idx, 'Classifier']
best_clf = classifiers[best_name]

print(f"\nBest classifier for 3-class problem: {best_name}")

# Fit best pipeline to visualize CSP patterns
best_pipeline = Pipeline([
    ('CSP', CSP(n_components=n_components, reg=None, log=True, norm_trace=False)),
    ('Classifier', best_clf)
])
best_pipeline.fit(X_train, y_train)

# Plot CSP patterns
#best_pipeline.named_steps['CSP'].plot_patterns(last_epochs.info, ch_type='eeg', units='Patterns (AU)', size=1.5)
plt.suptitle(f'CSP Patterns ({best_name} Classifier, 3 Classes)')
plt.show()
